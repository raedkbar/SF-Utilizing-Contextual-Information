{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU1rf5RTofCZ",
        "outputId": "6293847a-4a94-47e0-886d-d0b33f8dda2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAeVP1Mqr9Ky",
        "outputId": "73a7865d-87d8-4136-b005-7af3cfe94ac9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting pytorch-crf\n",
            "  Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: pytorch-crf\n",
            "Successfully installed pytorch-crf-0.7.2\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (2.2.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext) (2.0.7)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->torchtext) (12.4.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->torchtext) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->torchtext) (1.3.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install pytorch-crf\n",
        "!pip install torchtext\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xRHzlGL1G95"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "\n",
        "\n",
        "from torchcrf import CRF\n",
        "from torchtext.vocab import GloVe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ac3O40jCY1GO",
        "outputId": "e0952216-eba1-46f6-d16f-6b35a378ba83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdgR-stHP6Ag",
        "outputId": "2a750b5b-b293-4bbe-a6a5-d69a2b90c2f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# Define device for training based on CUDA availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qr7dpWi3Jh7"
      },
      "outputs": [],
      "source": [
        "def extract_data(seq_in_path, seq_out_path, label_path):\n",
        "    \"\"\"Extracts texts, slot tags, labels, and POS tags from files.\"\"\"\n",
        "    with open(seq_in_path, 'r', encoding='utf-8') as seq_in_file, \\\n",
        "         open(seq_out_path, 'r', encoding='utf-8') as seq_out_file, \\\n",
        "         open(label_path, 'r', encoding='utf-8') as label_file:\n",
        "        texts = [line.strip().split() for line in seq_in_file.readlines()]\n",
        "        slot_tags = [line.strip().split() for line in seq_out_file.readlines()]\n",
        "        labels = [line.strip() for line in label_file.readlines()]\n",
        "\n",
        "    # Generate POS tags for each sentence in texts\n",
        "    pos_tags = [nltk.pos_tag(sentence) for sentence in texts]\n",
        "\n",
        "    # Extract just the tags, discarding the words\n",
        "    pos_tags_only = [[tag for word, tag in sentence] for sentence in pos_tags]\n",
        "\n",
        "    return texts, slot_tags, labels, pos_tags_only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrObCoAB3JuA"
      },
      "outputs": [],
      "source": [
        "def prepare_data(texts, slot_tags, pos_tags_only):\n",
        "    \"\"\"Prepares data by creating word, tag, and POS tag indices.\"\"\"\n",
        "    word_to_ix = {\"<PAD>\": 0}\n",
        "    tag_to_ix = {\"<PAD>\": 0}\n",
        "    pos_to_ix = {\"<PAD>\": 0}  # Initialize POS tags index dictionary\n",
        "\n",
        "    for sentence in texts:\n",
        "        for word in sentence:\n",
        "            if word not in word_to_ix:\n",
        "                word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "    for tags in slot_tags:\n",
        "        for tag in tags:\n",
        "            if tag not in tag_to_ix:\n",
        "                tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "    for pos_sentence in pos_tags_only:\n",
        "        for pos_tag in pos_sentence:\n",
        "            if pos_tag not in pos_to_ix:\n",
        "                pos_to_ix[pos_tag] = len(pos_to_ix)\n",
        "\n",
        "    return word_to_ix, tag_to_ix, pos_to_ix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSGWqX6n3KSl"
      },
      "outputs": [],
      "source": [
        "class DatasetObj(Dataset):\n",
        "    \"\"\"Custom Dataset class to handle data loading, including POS tags.\"\"\"\n",
        "    def __init__(self, texts, slot_tags, pos_tags, labels=None, word_to_ix=None, tag_to_ix=None, pos_to_ix=None):\n",
        "        self.texts = texts\n",
        "        self.slot_tags = slot_tags\n",
        "        self.pos_tags = pos_tags  # New: Store POS tags\n",
        "        self.labels = labels\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.pos_to_ix = pos_to_ix  # New: Store POS to index mapping\n",
        "        self.sentences = [[self.word_to_ix[word] for word in sentence] for sentence in texts]\n",
        "        self.tags = [[self.tag_to_ix[tag] for tag in slot_tag] for slot_tag in slot_tags]\n",
        "        self.pos = [[self.pos_to_ix[pos] for pos in pos_sentence] for pos_sentence in pos_tags]  # New: Convert POS tags to indices\n",
        "\n",
        "        if labels is not None:\n",
        "            self.label_to_ix = {label: i for i, label in enumerate(set(labels))}\n",
        "            self.labels_ix = [self.label_to_ix[label] for label in labels]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence_tensor = torch.tensor(self.sentences[idx], dtype=torch.long)\n",
        "        tags_tensor = torch.tensor(self.tags[idx], dtype=torch.long)\n",
        "        pos_tensor = torch.tensor(self.pos[idx], dtype=torch.long)  # New: Create a tensor for POS tags\n",
        "\n",
        "        if self.labels is not None:\n",
        "            label_tensor = torch.tensor(self.labels_ix[idx], dtype=torch.long)\n",
        "            return sentence_tensor, tags_tensor, pos_tensor, label_tensor  # New: Return POS tags tensor\n",
        "        else:\n",
        "            return sentence_tensor, tags_tensor, pos_tensor  # New: Include POS tensor in the return\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        sentences, tags, pos_tags, *optional_labels = zip(*batch)  # New: Extract POS tags from batch\n",
        "        sentences_padded = torch.nn.utils.rnn.pad_sequence(sentences, batch_first=True, padding_value=0)\n",
        "        tags_padded = torch.nn.utils.rnn.pad_sequence(tags, batch_first=True, padding_value=0)\n",
        "        pos_tags_padded = torch.nn.utils.rnn.pad_sequence(pos_tags, batch_first=True, padding_value=0)  # New: Pad POS tags\n",
        "\n",
        "        if optional_labels:\n",
        "            labels = torch.tensor(optional_labels[0], dtype=torch.long)\n",
        "            return sentences_padded, tags_padded, pos_tags_padded, labels  # Return padded POS tags\n",
        "        else:\n",
        "            return sentences_padded, tags_padded, pos_tags_padded  # Include POS padding in return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r6NguiPmz8J"
      },
      "outputs": [],
      "source": [
        "# Choose dataset (atis, snips):\n",
        "ds = \"atis\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rG0Yjuj8ml26",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Paths for the training dataset\n",
        "train_seq_in_path = f'/content/drive/MyDrive/data/{ds}/train/seq.in'\n",
        "train_seq_out_path = f'/content/drive/MyDrive/data/{ds}/train/seq.out'\n",
        "train_label_path = f'/content/drive/MyDrive/data/{ds}/train/label'\n",
        "\n",
        "# Paths for the development dataset\n",
        "dev_seq_in_path = f'/content/drive/MyDrive/data/{ds}/dev/seq.in'\n",
        "dev_seq_out_path = f'/content/drive/MyDrive/data/{ds}/dev/seq.out'\n",
        "dev_label_path = f'/content/drive/MyDrive/data/{ds}/dev/label'\n",
        "\n",
        "# Paths for the test dataset\n",
        "test_seq_in_path = f'/content/drive/MyDrive/data/{ds}/test/seq.in'\n",
        "test_seq_out_path = f'/content/drive/MyDrive/data/{ds}/test/seq.out'\n",
        "test_label_path = f'/content/drive/MyDrive/data/{ds}/test/label'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVZ6J4a7orpJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7da6ed27-0e50-4b09-cc57-1ab90e376733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "{'<PAD>': 0, 'NN': 1, 'VBP': 2, 'TO': 3, 'VB': 4, 'IN': 5, 'NNS': 6, 'JJR': 7, 'CD': 8, 'JJ': 9, 'PRP': 10, 'DT': 11, 'VBG': 12, 'WDT': 13, 'CC': 14, 'VBZ': 15, 'RB': 16, 'WRB': 17, 'MD': 18, 'RP': 19, 'PDT': 20, 'WP': 21, 'JJS': 22, 'VBN': 23, 'PRP$': 24, 'EX': 25, 'VBD': 26, 'FW': 27, 'RBS': 28, 'UH': 29, 'NNP': 30, 'RBR': 31}\n"
          ]
        }
      ],
      "source": [
        "# Extract data for each split\n",
        "train_texts, train_slot_tags, train_labels, train_pos_tags = extract_data(train_seq_in_path, train_seq_out_path, train_label_path)\n",
        "dev_texts, dev_slot_tags, dev_labels, dev_pos_tags = extract_data(dev_seq_in_path, dev_seq_out_path, dev_label_path)\n",
        "test_texts, test_slot_tags, test_labels, test_pos_tags = extract_data(test_seq_in_path, test_seq_out_path, test_label_path)\n",
        "\n",
        "all_texts = train_texts + dev_texts + test_texts\n",
        "all_slot_tags = train_slot_tags + dev_slot_tags + test_slot_tags\n",
        "all_pos_tags = train_pos_tags + dev_pos_tags + test_pos_tags  # Combine all POS tags\n",
        "\n",
        "# Adjust the call to prepare_data to include POS tags\n",
        "word_to_ix, tag_to_ix, pos_to_ix = prepare_data(all_texts, all_slot_tags, all_pos_tags)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NI_6WuOOSn-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b90c187a-928e-403f-e6a5-c84c3241b02f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:39, 5.39MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:54<00:00, 7344.18it/s]\n"
          ]
        }
      ],
      "source": [
        "glove = GloVe(name='6B', dim=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpEqXq6VSqPq"
      },
      "outputs": [],
      "source": [
        "def create_embedding_matrix(word_to_ix, glove):\n",
        "    \"\"\"Creates an embedding matrix for the vocabulary.\"\"\"\n",
        "    embedding_dim = glove.dim\n",
        "    embeddings = torch.randn(len(word_to_ix), embedding_dim)\n",
        "    embeddings[word_to_ix[\"<PAD>\"]] = torch.zeros(embedding_dim)\n",
        "    for word, ix in word_to_ix.items():\n",
        "        if word in glove.stoi:\n",
        "            embeddings[ix] = glove[word]\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def create_pos_embedding_matrix(pos_to_ix, pos_embedding_dim):\n",
        "    \"\"\"Creates an embedding matrix for POS tags.\"\"\"\n",
        "    embeddings = torch.randn(len(pos_to_ix), pos_embedding_dim)\n",
        "    embeddings[pos_to_ix[\"<PAD>\"]] = torch.zeros(pos_embedding_dim)  # Zero vector for padding\n",
        "    return embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QCrMVoJLrfe"
      },
      "outputs": [],
      "source": [
        "class SentenceLevelPrediction(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_labels):\n",
        "        super(SentenceLevelPrediction, self).__init__()\n",
        "        self.pooling = nn.AdaptiveMaxPool1d(1)\n",
        "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
        "\n",
        "    def forward(self, lstm_out):\n",
        "        pooled = self.pooling(lstm_out.transpose(1, 2)).squeeze(-1)\n",
        "        logits = self.fc(pooled)\n",
        "        return torch.sigmoid(logits)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMwC-PuaLADm"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim * 2, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pkk73YzG3K3S"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim, pretrained_word_embeddings=None, pretrained_pos_embeddings=None, pos_vocab_size=None, pos_embedding_dim=None, num_labels=None):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # For word embeddings\n",
        "        self.word_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "\n",
        "        # For POS embeddings\n",
        "        self.pos_embedding = nn.Embedding(pos_vocab_size, pos_embedding_dim)\n",
        "\n",
        "        # Adjust LSTM input size to sum of word and POS embedding dimensions\n",
        "        lstm_input_dim = embedding_dim + pos_embedding_dim\n",
        "        self.lstm = nn.LSTM(lstm_input_dim, hidden_dim // 2, num_layers=1, bidirectional=True)\n",
        "\n",
        "\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "        self.crf = CRF(tagset_size, batch_first=True)\n",
        "\n",
        "        # Adversarial MI Estimation Components\n",
        "        self.discriminator = Discriminator(hidden_dim)\n",
        "\n",
        "        # Initialize the sentence-level prediction module if num_labels is provided\n",
        "        if num_labels is not None:\n",
        "            self.sentence_level_predictor = nn.Linear(hidden_dim, num_labels)\n",
        "\n",
        "        # Auxiliary components\n",
        "        self.word_context_classifier = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "        self.context2label = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "        self.sentence_label_classifier = nn.Linear(hidden_dim, tagset_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, sentence,pos_tags):\n",
        "        word_embeds = self.word_embedding(sentence)\n",
        "        pos_embeds = self.pos_embedding(pos_tags)\n",
        "        embeds = torch.cat((word_embeds, pos_embeds), dim=-1)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        tag_space = self.hidden2tag(lstm_out)\n",
        "\n",
        "        sentence_representation = torch.mean(lstm_out, dim=1)\n",
        "        sentence_level_logits = None\n",
        "        if hasattr(self, 'sentence_level_predictor'):\n",
        "            sentence_level_logits = torch.sigmoid(self.sentence_level_predictor(sentence_representation))\n",
        "\n",
        "        word_labels_from_context = self.context2label(torch.mean(lstm_out, dim=1))\n",
        "\n",
        "        return tag_space, lstm_out, sentence_level_logits, word_labels_from_context\n",
        "\n",
        "    def compute_discriminator_loss(self, lstm_out, device):\n",
        "        batch_size, seq_len, hidden_dim = lstm_out.size()\n",
        "\n",
        "        # context vector generation\n",
        "        context_vectors = lstm_out.mean(dim=1, keepdim=True).expand(-1, seq_len, -1)\n",
        "\n",
        "        joint_samples = torch.cat((lstm_out, context_vectors), dim=-1)\n",
        "\n",
        "        # Randomly shuffle context vectors\n",
        "        idx = torch.randperm(batch_size)\n",
        "        marginal_context_vectors = context_vectors[idx]\n",
        "\n",
        "        marginal_samples = torch.cat((lstm_out, marginal_context_vectors), dim=-1)\n",
        "\n",
        "        # Compute discriminator loss\n",
        "        true_preds = self.discriminator(joint_samples.view(-1, hidden_dim * 2))\n",
        "        false_preds = self.discriminator(marginal_samples.view(-1, hidden_dim * 2))\n",
        "\n",
        "        true_labels = torch.ones(true_preds.size(), device=device)\n",
        "        false_labels = torch.zeros(false_preds.size(), device=device)\n",
        "\n",
        "        disc_loss = F.binary_cross_entropy(torch.cat((true_preds, false_preds), dim=0),\n",
        "                                           torch.cat((true_labels, false_labels), dim=0))\n",
        "\n",
        "        return disc_loss\n",
        "\n",
        "    def compute_auxiliary_losses(self, lstm_out, tags, sentence_lengths, labels=None):\n",
        "        # Initialization of loss components\n",
        "        auxiliary_loss = 0\n",
        "\n",
        "        lstm_out_flat = lstm_out.contiguous().view(-1, lstm_out.shape[-1])\n",
        "        tags_flat = tags.contiguous().view(-1)\n",
        "        word_context_logits = self.word_context_classifier(lstm_out_flat)\n",
        "        word_context_loss = nn.CrossEntropyLoss()(word_context_logits, tags_flat)\n",
        "        auxiliary_loss += word_context_loss\n",
        "\n",
        "        if labels is not None and hasattr(self, 'sentence_level_predictor'):\n",
        "            sentence_representation = torch.mean(lstm_out, dim=1)\n",
        "            sentence_label_logits = self.sentence_level_predictor(sentence_representation)\n",
        "            sentence_label_loss = nn.BCEWithLogitsLoss()(sentence_label_logits, labels.float())\n",
        "            auxiliary_loss += sentence_label_loss\n",
        "\n",
        "        return auxiliary_loss\n",
        "\n",
        "\n",
        "    def loss(self, tag_space, lstm_out, sentence_level_logits, word_labels_from_context, tags, alpha, beta, gamma, labels=None):\n",
        "        crf_loss = -self.crf(tag_space, tags, mask=(tags != 0), reduction='mean')\n",
        "        disc_loss = self.compute_discriminator_loss(lstm_out, device)\n",
        "\n",
        "        auxiliary_loss = 0\n",
        "        if labels is not None:\n",
        "            if sentence_level_logits is not None:\n",
        "                sentence_level_loss = F.binary_cross_entropy_with_logits(sentence_level_logits, labels.float())\n",
        "                auxiliary_loss += sentence_level_loss\n",
        "\n",
        "        total_loss = alpha * crf_loss + beta * disc_loss + gamma * auxiliary_loss\n",
        "        return total_loss\n",
        "\n",
        "\n",
        "    def predict(self, sentences,pos_tags):\n",
        "        tag_space, _, _, _ = self.forward(sentences,pos_tags)\n",
        "        tags = self.crf.decode(tag_space)\n",
        "        return tags\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-LytzJdASFf"
      },
      "outputs": [],
      "source": [
        "def train_and_validate_model(model, train_loader, dev_loader, optimizer, num_epochs, device, alpha, beta, gamma):\n",
        "    best_validation_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for sentences, pos_tags, tags, labels in train_loader:  # Adjusted to unpack pos_tags\n",
        "            sentences, pos_tags, tags = sentences.to(device), pos_tags.to(device), tags.to(device)\n",
        "            labels = labels.to(device) if labels is not None else None\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Call the model's forward method with both sentences and pos_tags\n",
        "            outputs = model(sentences, pos_tags)\n",
        "            tag_space, lstm_out, sentence_level_logits, word_labels_from_context = outputs\n",
        "\n",
        "\n",
        "            # Use the outputs in the model's loss function\n",
        "            loss = model.loss(tag_space=tag_space, lstm_out=lstm_out, sentence_level_logits=sentence_level_logits,\n",
        "                  word_labels_from_context=word_labels_from_context, tags=tags, alpha=alpha, beta=beta, gamma=gamma, labels=labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        # Calculate average train loss\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        total_validation_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for sentences, pos_tags, tags, labels in dev_loader:  # Adjusted to unpack pos_tags\n",
        "                sentences, pos_tags, tags = sentences.to(device), pos_tags.to(device), tags.to(device)\n",
        "                labels = labels.to(device) if labels is not None else None\n",
        "\n",
        "                # Compute the loss for validation, similarly updated\n",
        "                tag_space, lstm_out, sentence_level_logits, word_labels_from_context = model(sentences, pos_tags)\n",
        "                val_loss = model.loss(tag_space, lstm_out, sentence_level_logits,\n",
        "                                      word_labels_from_context,tags, alpha, beta, gamma, labels=labels)  # Adjusted for validation\n",
        "                total_validation_loss += val_loss.item()\n",
        "\n",
        "        # Calculate average validation loss\n",
        "        avg_validation_loss = total_validation_loss / len(dev_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_validation_loss:.4f}\")\n",
        "\n",
        "        # Save the model if validation loss has improved\n",
        "        if avg_validation_loss < best_validation_loss:\n",
        "            best_validation_loss = avg_validation_loss\n",
        "            torch.save(model.state_dict(), f'model_best_validation.pth')\n",
        "            print(\"Model saved with improved validation loss.\")\n",
        "\n",
        "    return best_validation_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPGMuisSdNAO"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    true_tags, pred_tags = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sentences, pos_tags, tags, additional_data in test_loader:\n",
        "            sentences, pos_tags, tags = sentences.to(device), pos_tags.to(device), tags.to(device)\n",
        "            predicted_tags_batch = model.predict(sentences, pos_tags)\n",
        "            predicted_tags_batch = torch.tensor(predicted_tags_batch, dtype=torch.long, device=device)\n",
        "            predicted_tags_batch = predicted_tags_batch.view(-1).cpu().numpy()\n",
        "\n",
        "            true_tags.extend(tags.view(-1).cpu().numpy())\n",
        "            pred_tags.extend(predicted_tags_batch)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(true_tags, pred_tags)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(true_tags, pred_tags, average='macro', zero_division=1)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_max_pos_vocab_size(data_loaders):\n",
        "    max_index = -1\n",
        "    for loader in data_loaders:\n",
        "        for batch in loader:\n",
        "            # Use index-based access for pos_tags\n",
        "            pos_tags = batch[1]\n",
        "            current_max = pos_tags.max().item()\n",
        "            max_index = max(max_index, current_max)\n",
        "    # Since indices are zero-based, add 1 to get the correct vocabulary size.\n",
        "    pos_vocab_size = max_index + 1\n",
        "    return pos_vocab_size\n"
      ],
      "metadata": {
        "id": "WsE4mgU4ZnTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hs_SNF53OkVy"
      },
      "outputs": [],
      "source": [
        "# Dimension for POS tag embeddings\n",
        "pos_embedding_dim = 50\n",
        "pretrained_embeddings = create_embedding_matrix(word_to_ix, glove)\n",
        "pos_embeddings = create_pos_embedding_matrix(pos_to_ix, pos_embedding_dim)  # POS tag embeddings\n",
        "\n",
        "max_index = max(pos_to_ix.values())\n",
        "\n",
        "vocab_size = len(word_to_ix)\n",
        "tagset_size = len(tag_to_ix)\n",
        "\n",
        "embedding_dim = glove.dim\n",
        "hidden_dim = 200\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_epochs = 15\n",
        "batch_size = 1\n",
        "\n",
        "tradeoff_params = {'alpha': .1, 'beta': .1, 'gamma': .1}\n",
        "\n",
        "best_val_loss = float('inf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaeeaiCVhICN"
      },
      "outputs": [],
      "source": [
        "train_dataset = DatasetObj(train_texts, train_slot_tags, train_pos_tags, train_labels, word_to_ix, tag_to_ix, pos_to_ix)\n",
        "dev_dataset = DatasetObj(dev_texts, dev_slot_tags, dev_pos_tags, dev_labels, word_to_ix, tag_to_ix, pos_to_ix)\n",
        "test_dataset = DatasetObj(test_texts, test_slot_tags, test_pos_tags, test_labels, word_to_ix, tag_to_ix, pos_to_ix)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=DatasetObj.collate_fn)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=DatasetObj.collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=DatasetObj.collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfCGecCUhIpY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b26caf88-1b9e-4afd-c0b0-038e3692afbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 0.4251, Validation Loss: 0.2800\n",
            "Model saved with improved validation loss.\n",
            "Epoch 2, Train Loss: 0.2355, Validation Loss: 0.2440\n",
            "Model saved with improved validation loss.\n",
            "Epoch 3, Train Loss: 0.2072, Validation Loss: 0.2317\n",
            "Model saved with improved validation loss.\n",
            "Epoch 4, Train Loss: 0.1952, Validation Loss: 0.2243\n",
            "Model saved with improved validation loss.\n",
            "Epoch 5, Train Loss: 0.1884, Validation Loss: 0.2219\n",
            "Model saved with improved validation loss.\n",
            "Epoch 6, Train Loss: 0.1834, Validation Loss: 0.2196\n",
            "Model saved with improved validation loss.\n",
            "Epoch 7, Train Loss: 0.1810, Validation Loss: 0.2230\n",
            "Epoch 8, Train Loss: 0.1781, Validation Loss: 0.2284\n",
            "Epoch 9, Train Loss: 0.1769, Validation Loss: 0.2214\n",
            "Epoch 10, Train Loss: 0.1753, Validation Loss: 0.2287\n",
            "Epoch 11, Train Loss: 0.1746, Validation Loss: 0.2208\n",
            "Epoch 12, Train Loss: 0.1732, Validation Loss: 0.2281\n",
            "Epoch 13, Train Loss: 0.1730, Validation Loss: 0.2305\n",
            "Epoch 14, Train Loss: 0.1721, Validation Loss: 0.2292\n",
            "Epoch 15, Train Loss: 0.1713, Validation Loss: 0.2277\n",
            "Best Validation Loss: 0.21959799146652223\n"
          ]
        }
      ],
      "source": [
        "pos_vocab_size = calculate_max_pos_vocab_size([train_loader, dev_loader, test_loader])\n",
        "model = Model(vocab_size=vocab_size, tagset_size=tagset_size, embedding_dim=embedding_dim, hidden_dim=hidden_dim,\n",
        "              pretrained_word_embeddings=pretrained_embeddings, pretrained_pos_embeddings=pos_embeddings,\n",
        "              pos_vocab_size=pos_vocab_size, pos_embedding_dim=pos_embedding_dim).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "val_loss = train_and_validate_model(model, train_loader, dev_loader, optimizer, num_epochs, device, **tradeoff_params)\n",
        "\n",
        "if val_loss < best_val_loss:\n",
        "   best_val_loss = val_loss\n",
        "\n",
        "print(f\"Best Validation Loss: {best_val_loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbfDULgBASB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f6e9076-c1f7-4a0c-f5aa-85e111842a39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchcrf/__init__.py:305: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:519.)\n",
            "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9470\n",
            "Precision: 0.9012, Recall: 0.7854, F1: 0.7994\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(model, test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}