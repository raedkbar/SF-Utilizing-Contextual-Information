{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU1rf5RTofCZ",
        "outputId": "661d111b-3f43-49ee-9eca-39e2eb2515c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installing Required Libraries\n",
        "\n",
        "Install the necessary Python packages for the project.\n"
      ],
      "metadata": {
        "id": "amJhPmGIV2k9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAeVP1Mqr9Ky",
        "outputId": "55dd7956-b9dd-4c19-ea18-4b87af7f1595"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting pytorch-crf\n",
            "  Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: pytorch-crf\n",
            "Successfully installed pytorch-crf-0.7.2\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (2.2.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext) (2.0.7)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->torchtext) (12.4.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->torchtext) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->torchtext) (1.3.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install pytorch-crf\n",
        "!pip install torchtext\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Libraries\n",
        "Import the required libraries and modules for the project."
      ],
      "metadata": {
        "id": "SgbjYlBfWPdH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "2xRHzlGL1G95"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "\n",
        "\n",
        "from torchcrf import CRF\n",
        "from torchtext.vocab import GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Downloading NLTK Resources\n",
        "\n",
        "Download necessary resources from the NLTK package:\n",
        "\n",
        "the punkt sentence tokenizer and the averaged_perceptron_tagger for part-of-speech (POS) tagging. These resources are essential for preprocessing text data."
      ],
      "metadata": {
        "id": "LSqWMMJkWaLA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ac3O40jCY1GO",
        "outputId": "712dfc26-46cd-4b1d-fac7-fdb4df97b8a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "# Downloading necessary NLTK resources for tokenization and POS tagging\n",
        "nltk.download('punkt')  # Tokenizer for sentence splitting\n",
        "nltk.download('averaged_perceptron_tagger')  # POS tagger\n",
        "from nltk.tokenize import word_tokenize  # Tokenizer function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setting Up the Device\n",
        "\n",
        "Determine if a CUDA-capable GPU is available for PyTorch to use (cuda), otherwise defaults to using the CPU (cpu)."
      ],
      "metadata": {
        "id": "WaYbwRiJW2zI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdgR-stHP6Ag",
        "outputId": "6899d8a2-92f4-4358-c769-4ae3393c9a45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(torch.cuda.is_available())  # Check if CUDA (GPU support) is enabled"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Extraction Function\n",
        "\n",
        "A function, extract_data, that reads text, slot tags, and labels from specified file paths. It also generates POS tags for each sentence, creating a comprehensive dataset for the model to train on. The function returns four lists containing sentences, slot tags, intent labels, and POS tags."
      ],
      "metadata": {
        "id": "x9jW4LOyXE6W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "8qr7dpWi3Jh7"
      },
      "outputs": [],
      "source": [
        "def extract_data(seq_in_path, seq_out_path, label_path):\n",
        "    # Reading data from files\n",
        "    with open(seq_in_path, 'r', encoding='utf-8') as seq_in_file, \\\n",
        "         open(seq_out_path, 'r', encoding='utf-8') as seq_out_file, \\\n",
        "         open(label_path, 'r', encoding='utf-8') as label_file:\n",
        "        texts = [line.strip().split() for line in seq_in_file.readlines()]\n",
        "        slot_tags = [line.strip().split() for line in seq_out_file.readlines()]\n",
        "        labels = [line.strip() for line in label_file.readlines()]\n",
        "\n",
        "    # Generating POS tags for each sentence\n",
        "    pos_tags = [nltk.pos_tag(sentence) for sentence in texts]\n",
        "    # Extracting just the POS tags\n",
        "    pos_tags_only = [[tag for word, tag in sentence] for sentence in pos_tags]\n",
        "\n",
        "    return texts, slot_tags, labels, pos_tags_only"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preparation Function\n",
        "\n",
        "prepare_data takes text data and converts words, slot tags, POS tags, and labels into numeric indices, facilitating their use in neural network models. It creates mappings (word_to_ix, tag_to_ix, pos_to_ix, label_to_ix) for each unique word, tag, POS tag, and label to an index."
      ],
      "metadata": {
        "id": "KRzNgZ-9Xgwr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "xrObCoAB3JuA"
      },
      "outputs": [],
      "source": [
        "def prepare_data(texts, slot_tags, pos_tags_only, labels):\n",
        "    word_to_ix = {\"<PAD>\": 0}  # Mapping words to indices, with padding token\n",
        "    tag_to_ix = {\"<PAD>\": 0}  # Mapping slot tags to indices\n",
        "    pos_to_ix = {\"<PAD>\": 0}  # Mapping POS tags to indices\n",
        "    label_to_ix = {\"<PAD>\": 0}  # Mapping labels to indices\n",
        "\n",
        "    # Populating mappings\n",
        "    for sentence in texts:\n",
        "        for word in sentence:\n",
        "            if word not in word_to_ix:\n",
        "                word_to_ix[word] = len(word_to_ix)\n",
        "    for tags in slot_tags:\n",
        "        for tag in tags:\n",
        "            if tag not in tag_to_ix:\n",
        "                tag_to_ix[tag] = len(tag_to_ix)\n",
        "    for pos_sentence in pos_tags_only:\n",
        "        for pos_tag in pos_sentence:\n",
        "            if pos_tag not in pos_to_ix:\n",
        "                pos_to_ix[pos_tag] = len(pos_to_ix)\n",
        "    for label in labels:\n",
        "        if label not in label_to_ix:\n",
        "            label_to_ix[label] = len(label_to_ix)\n",
        "\n",
        "    return word_to_ix, tag_to_ix, pos_to_ix, label_to_ix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset Object Definition\n",
        "\n",
        "A custom PyTorch Dataset class, DatasetObj, to handle loading and batching the data efficiently. It includes the capability to work with text data, slot tags, POS tags, and labels. The class also defines a collate_fn method for batching, padding sequences to a uniform length within each batch."
      ],
      "metadata": {
        "id": "XynuSt4ZXkz8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "zSGWqX6n3KSl"
      },
      "outputs": [],
      "source": [
        "class DatasetObj(Dataset):\n",
        "    def __init__(self, texts, slot_tags, pos_tags, labels=None, word_to_ix=None, tag_to_ix=None, pos_to_ix=None):\n",
        "        self.texts = texts  # Sentences\n",
        "        self.slot_tags = slot_tags  # Slot tags for each token in sentences\n",
        "        self.pos_tags = pos_tags  # POS tags for each token\n",
        "        self.labels = labels  # Intent labels for sentences\n",
        "        self.word_to_ix = word_to_ix  # Word to index mapping\n",
        "        self.tag_to_ix = tag_to_ix  # Slot tag to index mapping\n",
        "        self.pos_to_ix = pos_to_ix  # POS tag to index mapping\n",
        "        # Converting texts and tags to their respective indices\n",
        "        self.sentences = [[self.word_to_ix[word] for word in sentence] for sentence in texts]\n",
        "        self.tags = [[self.tag_to_ix[tag] for tag in slot_tag] for slot_tag in slot_tags]\n",
        "        self.pos = [[self.pos_to_ix[pos] for pos in pos_sentence] for pos_sentence in pos_tags]\n",
        "        if labels is not None:\n",
        "            self.label_to_ix = {label: i for i, label in enumerate(set(labels))}\n",
        "            self.labels_ix = [self.label_to_ix[label] for label in labels]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence_tensor = torch.tensor(self.sentences[idx], dtype=torch.long)\n",
        "        tags_tensor = torch.tensor(self.tags[idx], dtype=torch.long)\n",
        "        pos_tensor = torch.tensor(self.pos[idx], dtype=torch.long)\n",
        "        if self.labels is not None:\n",
        "            label_tensor = torch.tensor(self.labels_ix[idx], dtype=torch.long)\n",
        "            return sentence_tensor, tags_tensor, pos_tensor, label_tensor\n",
        "        else:\n",
        "            return sentence_tensor, tags_tensor, pos_tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        sentences, tags, pos_tags, *optional_labels = zip(*batch)\n",
        "        sentences_padded = torch.nn.utils.rnn.pad_sequence(sentences, batch_first=True, padding_value=0)\n",
        "        tags_padded = torch.nn.utils.rnn.pad_sequence(tags, batch_first=True, padding_value=0)\n",
        "        pos_tags_padded = torch.nn.utils.rnn.pad_sequence(pos_tags, batch_first=True, padding_value=0)\n",
        "        if optional_labels:\n",
        "            labels = torch.tensor(optional_labels[0], dtype=torch.long)\n",
        "            return sentences_padded, tags_padded, pos_tags_padded, labels\n",
        "        else:\n",
        "            return sentences_padded, tags_padded, pos_tags_padded"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset Selection\n",
        "\n",
        "Sets the dataset to use (atis or snips)."
      ],
      "metadata": {
        "id": "v-iFIpIfXr5i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "1r6NguiPmz8J"
      },
      "outputs": [],
      "source": [
        "# Specify the dataset to use (e.g., \"atis\" or \"snips\")\n",
        "ds = \"atis\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Path Specification\n",
        "\n",
        "Specifies the file paths for the training, development, and test datasets."
      ],
      "metadata": {
        "id": "vnZYmw-9XvhX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "rG0Yjuj8ml26",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Paths for the training dataset\n",
        "train_seq_in_path = f'/content/drive/MyDrive/data/{ds}/train/seq.in'\n",
        "train_seq_out_path = f'/content/drive/MyDrive/data/{ds}/train/seq.out'\n",
        "train_label_path = f'/content/drive/MyDrive/data/{ds}/train/label'\n",
        "\n",
        "# Paths for the development dataset\n",
        "dev_seq_in_path = f'/content/drive/MyDrive/data/{ds}/dev/seq.in'\n",
        "dev_seq_out_path = f'/content/drive/MyDrive/data/{ds}/dev/seq.out'\n",
        "dev_label_path = f'/content/drive/MyDrive/data/{ds}/dev/label'\n",
        "\n",
        "# Paths for the test dataset\n",
        "test_seq_in_path = f'/content/drive/MyDrive/data/{ds}/test/seq.in'\n",
        "test_seq_out_path = f'/content/drive/MyDrive/data/{ds}/test/seq.out'\n",
        "test_label_path = f'/content/drive/MyDrive/data/{ds}/test/label'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loading\n",
        "\n",
        "Loads the training, development, and test data using the extract_data function defined earlier. It concatenates all texts, slot tags, POS tags, and labels from the different splits to create comprehensive mappings for the entire dataset."
      ],
      "metadata": {
        "id": "UCS22syoYD_4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "iVZ6J4a7orpJ"
      },
      "outputs": [],
      "source": [
        "# Extract sentences, slot tags, labels, and POS tags from the specified dataset files\n",
        "train_texts, train_slot_tags, train_labels, train_pos_tags = extract_data(train_seq_in_path, train_seq_out_path, train_label_path)\n",
        "dev_texts, dev_slot_tags, dev_labels, dev_pos_tags = extract_data(dev_seq_in_path, dev_seq_out_path, dev_label_path)\n",
        "test_texts, test_slot_tags, test_labels, test_pos_tags = extract_data(test_seq_in_path, test_seq_out_path, test_label_path)\n",
        "\n",
        "# Combine texts, slot tags, and POS tags from all data splits for vocabulary creation\n",
        "all_texts = train_texts + dev_texts + test_texts\n",
        "all_slot_tags = train_slot_tags + dev_slot_tags + test_slot_tags\n",
        "all_pos_tags = train_pos_tags + dev_pos_tags + test_pos_tags\n",
        "all_labels = train_labels + dev_labels + test_labels\n",
        "\n",
        "# Prepare data by creating mappings from words, tags, and labels to indices\n",
        "word_to_ix, tag_to_ix, pos_to_ix, label_to_ix = prepare_data(all_texts, all_slot_tags, all_pos_tags, all_labels)\n",
        "\n",
        "# Calculate the number of unique labels\n",
        "num_labels = len(set(all_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GloVe Embeddings Initialization\n",
        "\n",
        "Initializes the GloVe word embeddings from the torchtext library, specifying the 6B version with 300-dimensional vectors. These pre-trained embeddings will be used to represent words numerically."
      ],
      "metadata": {
        "id": "thsd2eh1YHAk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "NI_6WuOOSn-A"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained GloVe word embeddings with 300 dimensions\n",
        "glove = GloVe(name='6B', dim=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#embedding Matrices Creation\n",
        "\n",
        "Defines functions to create embedding matrices for both words (create_embedding_matrix) and POS tags (create_pos_embedding_matrix) based on the mappings created earlier. These matrices serve as the initial weights for the embedding layers in the model."
      ],
      "metadata": {
        "id": "_aWaIeACYJnA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "CpEqXq6VSqPq"
      },
      "outputs": [],
      "source": [
        "def create_embedding_matrix(word_to_ix, glove):\n",
        "    \"\"\"Creates an embedding matrix for the vocabulary.\"\"\"\n",
        "    embedding_dim = glove.dim\n",
        "    embeddings = torch.randn(len(word_to_ix), embedding_dim)\n",
        "    embeddings[word_to_ix[\"<PAD>\"]] = torch.zeros(embedding_dim)\n",
        "    for word, ix in word_to_ix.items():\n",
        "        if word in glove.stoi:\n",
        "            embeddings[ix] = glove[word]\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def create_pos_embedding_matrix(pos_to_ix, pos_embedding_dim):\n",
        "    \"\"\"Creates an embedding matrix for POS tags.\"\"\"\n",
        "    embeddings = torch.randn(len(pos_to_ix), pos_embedding_dim)\n",
        "    embeddings[pos_to_ix[\"<PAD>\"]] = torch.zeros(pos_embedding_dim)  # Zero vector for padding\n",
        "    return embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence-Level Prediction Module\n",
        "\n",
        "Defines a neural network module, SentenceLevelPrediction, that takes LSTM outputs, applies adaptive max pooling and a linear layer to predict sentence-level intents or other attributes."
      ],
      "metadata": {
        "id": "U9-4r_iIYL6k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "2QCrMVoJLrfe"
      },
      "outputs": [],
      "source": [
        "class SentenceLevelPrediction(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_labels):\n",
        "        super(SentenceLevelPrediction, self).__init__()\n",
        "        self.pooling = nn.AdaptiveMaxPool1d(1)  # Adaptive max pooling to a single value\n",
        "        self.fc = nn.Linear(hidden_dim, num_labels)  # Linear layer for label prediction\n",
        "\n",
        "    def forward(self, lstm_out):\n",
        "        # Apply adaptive max pooling and linear layer to LSTM output for sentence-level prediction\n",
        "        pooled = self.pooling(lstm_out.transpose(1, 2)).squeeze(-1)\n",
        "        logits = self.fc(pooled)\n",
        "        return torch.sigmoid(logits)  # Use sigmoid for binary/multi-label classification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Discriminator Definition\n",
        "\n",
        "Defines a Discriminator neural network for adversarial training. It aims to distinguish between real and fake (shuffled) sequences based on their representations, contributing to the mutual information maximization component of the model."
      ],
      "metadata": {
        "id": "17MmaOwsYPxI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "NMwC-PuaLADm"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim * 2, 128)  # First linear layer\n",
        "        self.fc2 = nn.Linear(128, 1)  # Second linear layer for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply ReLU activation followed by sigmoid for binary classification\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main Model Definition\n",
        "\n",
        "Defines the main model, Model, incorporating word and POS tag embeddings, an LSTM for sequence modeling, a CRF for sequence labeling, and various components for adversarial training and auxiliary tasks."
      ],
      "metadata": {
        "id": "YvniVy2IYYSa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pkk73YzG3K3S"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim, pretrained_word_embeddings=None, pretrained_pos_embeddings=None, pos_vocab_size=None, pos_embedding_dim=None, num_labels=None, num_lstm_layers=1):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # Initialize word embeddings from pretrained vectors or randomly.\n",
        "        self.word_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "        # Initialize POS tag embeddings.\n",
        "        self.pos_embedding = nn.Embedding(pos_vocab_size, pos_embedding_dim)\n",
        "        # Configure LSTM to process sequences, accounting for both word and POS embeddings.\n",
        "        self.lstm = nn.LSTM(embedding_dim + pos_embedding_dim, hidden_dim // 2, num_layers=num_lstm_layers, bidirectional=True)\n",
        "        # Linear layer to map LSTM output to tag space for sequence labeling.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "        # CRF layer for decoding the optimal tag sequence.\n",
        "        self.crf = CRF(tagset_size, batch_first=True)\n",
        "        # Discriminator for adversarial mutual information estimation.\n",
        "        self.discriminator = Discriminator(hidden_dim)\n",
        "        # Sentence-level classification head, if applicable.\n",
        "        self.sentence_level_predictor = nn.Linear(hidden_dim, num_labels)\n",
        "        # Auxiliary classification tasks for enhanced feature extraction.\n",
        "        self.word_context_classifier = nn.Linear(hidden_dim, tagset_size)\n",
        "        self.context2label = nn.Linear(hidden_dim, tagset_size)\n",
        "        self.sentence_label_classifier = nn.Linear(hidden_dim, num_labels)\n",
        "\n",
        "    def forward(self, sentence,pos_tags):\n",
        "        # Embedding lookup and combination for input sequences.\n",
        "        word_embeds = self.word_embedding(sentence)\n",
        "        pos_embeds = self.pos_embedding(pos_tags)\n",
        "        embeds = torch.cat((word_embeds, pos_embeds), dim=-1)\n",
        "        # Process sequence through LSTM.\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        # Map LSTM outputs to tag space and generate sentence-level predictions.\n",
        "        tag_space = self.hidden2tag(lstm_out)\n",
        "        sentence_representation = torch.mean(lstm_out, dim=1)\n",
        "        sentence_level_logits = None\n",
        "        if hasattr(self, 'sentence_level_predictor'):\n",
        "            sentence_level_logits = torch.sigmoid(self.sentence_level_predictor(sentence_representation))\n",
        "\n",
        "        word_labels_from_context = self.context2label(torch.mean(lstm_out, dim=1))\n",
        "\n",
        "        return tag_space, lstm_out, sentence_level_logits, word_labels_from_context\n",
        "\n",
        "    def compute_discriminator_loss(self, lstm_out, device):\n",
        "        # Generate context vectors and samples for adversarial training.\n",
        "        batch_size, seq_len, hidden_dim = lstm_out.size()\n",
        "\n",
        "        # context vector generation\n",
        "        context_vectors = lstm_out.mean(dim=1, keepdim=True).expand(-1, seq_len, -1)\n",
        "\n",
        "        joint_samples = torch.cat((lstm_out, context_vectors), dim=-1)\n",
        "\n",
        "        # Randomly shuffle context vectors\n",
        "        idx = torch.randperm(batch_size)\n",
        "        marginal_context_vectors = context_vectors[idx]\n",
        "\n",
        "        marginal_samples = torch.cat((lstm_out, marginal_context_vectors), dim=-1)\n",
        "\n",
        "        # Discriminator loss calculation for real and fake samples.\n",
        "        true_preds = self.discriminator(joint_samples.view(-1, hidden_dim * 2))\n",
        "        false_preds = self.discriminator(marginal_samples.view(-1, hidden_dim * 2))\n",
        "\n",
        "        true_labels = torch.ones(true_preds.size(), device=device)\n",
        "        false_labels = torch.zeros(false_preds.size(), device=device)\n",
        "\n",
        "        disc_loss = F.binary_cross_entropy(torch.cat((true_preds, false_preds), dim=0),\n",
        "                                           torch.cat((true_labels, false_labels), dim=0))\n",
        "\n",
        "        return disc_loss\n",
        "\n",
        "    def compute_auxiliary_losses(self, lstm_out, tags, sentence_lengths, labels=None):\n",
        "        # Calculate losses for auxiliary classification tasks.\n",
        "        auxiliary_loss = 0\n",
        "\n",
        "        lstm_out_flat = lstm_out.contiguous().view(-1, lstm_out.shape[-1])\n",
        "        tags_flat = tags.contiguous().view(-1)\n",
        "        word_context_logits = self.word_context_classifier(lstm_out_flat)\n",
        "        word_context_loss = nn.CrossEntropyLoss()(word_context_logits, tags_flat)\n",
        "        auxiliary_loss += word_context_loss\n",
        "\n",
        "        if labels is not None and hasattr(self, 'sentence_level_predictor'):\n",
        "            sentence_representation = torch.mean(lstm_out, dim=1)\n",
        "            sentence_label_logits = self.sentence_level_predictor(sentence_representation)\n",
        "            sentence_label_loss = nn.BCEWithLogitsLoss()(sentence_label_logits, labels.float())\n",
        "            auxiliary_loss += sentence_label_loss\n",
        "\n",
        "        return auxiliary_loss\n",
        "\n",
        "\n",
        "    def loss(self, tag_space, lstm_out, sentence_level_logits, word_labels_from_context, tags, alpha, beta, gamma, labels=None):\n",
        "        # Combine CRF loss, discriminator loss, and auxiliary losses into a total loss.\n",
        "        crf_loss = -self.crf(tag_space, tags, mask=(tags != 0), reduction='mean')\n",
        "        disc_loss = self.compute_discriminator_loss(lstm_out, device)\n",
        "\n",
        "        auxiliary_loss = 0\n",
        "\n",
        "        sentence_level_loss = F.cross_entropy(sentence_level_logits, labels)\n",
        "        auxiliary_loss += sentence_level_loss\n",
        "\n",
        "        total_loss = alpha * crf_loss + beta * disc_loss + gamma * auxiliary_loss\n",
        "        return total_loss\n",
        "\n",
        "    def predict(self, sentences, pos_tags):\n",
        "        # Predict slot tags and intent labels for given sequences.\n",
        "        tag_space, _, sentence_level_logits, _ = self.forward(sentences, pos_tags)\n",
        "        tags = self.crf.decode(tag_space)  # Slot predictions\n",
        "        intents = torch.argmax(sentence_level_logits, dim=1)  # Intent predictions\n",
        "\n",
        "        return tags, intents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Training and Validation\n",
        "\n",
        "Implements the function train_and_validate_model for training the model on the training dataset and validating its performance on the development dataset. It optimizes the model parameters based on a composite loss function and saves the best-performing model."
      ],
      "metadata": {
        "id": "qt77234MYcXb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "G-LytzJdASFf"
      },
      "outputs": [],
      "source": [
        "def train_and_validate_model(model, train_loader, dev_loader, optimizer, num_epochs, device, alpha, beta, gamma):\n",
        "    best_validation_loss = float('inf')  # Initialize the best validation loss to infinity for comparison later.\n",
        "\n",
        "    for epoch in range(num_epochs):  # Iterate over the specified number of epochs.\n",
        "        model.train()  # Set the model to training mode.\n",
        "        total_train_loss = 0  # Initialize total training loss for the epoch.\n",
        "\n",
        "        # Training phase\n",
        "        for sentences, pos_tags, tags, labels in train_loader:  # Loop through batches in the training data loader.\n",
        "            # Move data to the specified device (GPU or CPU).\n",
        "            sentences, pos_tags, tags = sentences.to(device), pos_tags.to(device), tags.to(device)\n",
        "            labels = labels.to(device) if labels is not None else None  # Labels might not always be provided.\n",
        "\n",
        "            optimizer.zero_grad()  # Clear gradients before computing them.\n",
        "\n",
        "            # Forward pass through the model.\n",
        "            tag_space, lstm_out, sentence_level_logits, word_labels_from_context = model(sentences, pos_tags)\n",
        "\n",
        "            # Calculate loss using the model's custom loss function.\n",
        "            loss = model.loss(tag_space=tag_space, lstm_out=lstm_out, sentence_level_logits=sentence_level_logits,\n",
        "                              word_labels_from_context=word_labels_from_context, tags=tags, alpha=alpha, beta=beta, gamma=gamma, labels=labels)\n",
        "\n",
        "            loss.backward()  # Perform backpropagation based on the loss.\n",
        "            optimizer.step()  # Update model parameters.\n",
        "\n",
        "            total_train_loss += loss.item()  # Accumulate total loss for the epoch.\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)  # Calculate average training loss.\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()  # Set the model to evaluation mode.\n",
        "        total_validation_loss = 0  # Initialize total validation loss for the epoch.\n",
        "        with torch.no_grad():  # Deactivate gradient calculations.\n",
        "            for sentences, pos_tags, tags, labels in dev_loader:  # Loop through batches in the development (validation) data loader.\n",
        "                # Move data to the device.\n",
        "                sentences, pos_tags, tags = sentences.to(device), pos_tags.to(device), tags.to(device)\n",
        "                labels = labels.to(device) if labels is not None else None\n",
        "\n",
        "                # Forward pass through the model without computing gradients.\n",
        "                tag_space, lstm_out, sentence_level_logits, word_labels_from_context = model(sentences, pos_tags)\n",
        "\n",
        "                # Calculate validation loss.\n",
        "                val_loss = model.loss(tag_space=tag_space, lstm_out=lstm_out, sentence_level_logits=sentence_level_logits,\n",
        "                                      word_labels_from_context=word_labels_from_context, tags=tags, alpha=alpha, beta=beta, gamma=gamma, labels=labels)\n",
        "                total_validation_loss += val_loss.item()\n",
        "\n",
        "        avg_validation_loss = total_validation_loss / len(dev_loader)  # Calculate average validation loss.\n",
        "\n",
        "        # Print training and validation loss for the epoch.\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_validation_loss:.4f}\")\n",
        "\n",
        "        # Check if the current epoch's validation loss is the best so far.\n",
        "        if avg_validation_loss < best_validation_loss:\n",
        "            best_validation_loss = avg_validation_loss  # Update best validation loss.\n",
        "            # Save the model state with the best validation loss.\n",
        "            torch.save(model.state_dict(), 'model_best_validation.pth')\n",
        "            print(\"Model saved with improved validation loss.\")\n",
        "\n",
        "    return best_validation_loss  # Return the best validation loss achieved."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Evaluation\n",
        "\n",
        "Defines evaluate_model, a function to evaluate the trained model on a test dataset. It calculates and prints out accuracy, precision, recall, and F1 scores for both slot filling and intent detection tasks."
      ],
      "metadata": {
        "id": "sq7GMB72Yfo5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "pPGMuisSdNAO"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    true_tags, pred_tags = [], []\n",
        "    true_intents, pred_intents = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sentences, pos_tags, tags, intent_labels in test_loader:\n",
        "            sentences, pos_tags, tags, intent_labels = sentences.to(device), pos_tags.to(device), tags.to(device), intent_labels.to(device)\n",
        "\n",
        "            # Assuming model.predict now also returns intent predictions\n",
        "            predicted_tags_batch, predicted_intents_batch = model.predict(sentences, pos_tags)\n",
        "\n",
        "            # Process slot filling predictions\n",
        "            predicted_tags_batch = torch.tensor(predicted_tags_batch, dtype=torch.long, device=device)\n",
        "            predicted_tags_batch = predicted_tags_batch.view(-1).cpu().numpy()\n",
        "            true_tags.extend(tags.view(-1).cpu().numpy())\n",
        "            pred_tags.extend(predicted_tags_batch)\n",
        "\n",
        "            # Process intent detection predictions\n",
        "            # Assuming predicted_intents_batch is a tensor of predicted intent indices\n",
        "            pred_intents.extend(predicted_intents_batch.cpu().numpy())\n",
        "            true_intents.extend(intent_labels.cpu().numpy())\n",
        "\n",
        "    # Calculate and print metrics for slot filling\n",
        "    accuracy_slots = accuracy_score(true_tags, pred_tags)\n",
        "    precision_slots, recall_slots, f1_slots, _ = precision_recall_fscore_support(true_tags, pred_tags, average='macro', zero_division=0)\n",
        "    print(f\"Slot Filling - Accuracy: {accuracy_slots:.4f}, Precision: {precision_slots:.4f}, Recall: {recall_slots:.4f}, F1: {f1_slots:.4f}\")\n",
        "\n",
        "    # Calculate and print metrics for intent detection\n",
        "    accuracy_intents = accuracy_score(true_intents, pred_intents)\n",
        "    precision_intents, recall_intents, f1_intents, _ = precision_recall_fscore_support(true_intents, pred_intents, average='macro', zero_division=0)\n",
        "    print(f\"Intent Detection - Accuracy: {accuracy_intents:.4f}, Precision: {precision_intents:.4f}, Recall: {recall_intents:.4f}, F1: {f1_intents:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Maximum POS Vocabulary Size Calculation\n",
        "\n",
        "Calculates the maximum POS tag index across all datasets to ensure the POS embedding layer accommodates all tags. This function helps define the correct size for the POS tag embedding matrix."
      ],
      "metadata": {
        "id": "F1P57CtrYo7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_max_pos_vocab_size(data_loaders):\n",
        "    max_index = -1\n",
        "    for loader in data_loaders:\n",
        "        for batch in loader:\n",
        "            # Use index-based access for pos_tags\n",
        "            pos_tags = batch[1]\n",
        "            current_max = pos_tags.max().item()\n",
        "            max_index = max(max_index, current_max)\n",
        "    # Since indices are zero-based, add 1 to get the correct vocabulary size.\n",
        "    pos_vocab_size = max_index + 1\n",
        "    return pos_vocab_size\n"
      ],
      "metadata": {
        "id": "WsE4mgU4ZnTM"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Hyperparameters and Initialization\n",
        "\n",
        "Sets various hyperparameters for the model, initializes the word and POS tag embedding matrices, and creates instances of the training, development, and test datasets and their corresponding data loaders."
      ],
      "metadata": {
        "id": "CWcwZ9S1YtN8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "Hs_SNF53OkVy"
      },
      "outputs": [],
      "source": [
        "# Dimension for POS tag embeddings\n",
        "pos_embedding_dim = 50\n",
        "pretrained_embeddings = create_embedding_matrix(word_to_ix, glove)\n",
        "pos_embeddings = create_pos_embedding_matrix(pos_to_ix, pos_embedding_dim)\n",
        "\n",
        "max_index = max(pos_to_ix.values())\n",
        "\n",
        "vocab_size = len(word_to_ix)\n",
        "tagset_size = len(tag_to_ix)\n",
        "\n",
        "embedding_dim = glove.dim\n",
        "hidden_dim = 100\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_epochs = 15\n",
        "batch_size = 1\n",
        "\n",
        "num_lstm_layers = 5\n",
        "\n",
        "tradeoff_params = {'alpha': .1, 'beta': .1, 'gamma': .1}\n",
        "\n",
        "best_val_loss = float('inf')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loader Creation\n",
        "\n",
        "Instantiates PyTorch DataLoader objects for the training, development, and test datasets, specifying batch sizes and whether to shuffle the data. These loaders streamline the process of feeding data into the model during training and evaluation."
      ],
      "metadata": {
        "id": "wkyAha2CYxap"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "QaeeaiCVhICN"
      },
      "outputs": [],
      "source": [
        "train_dataset = DatasetObj(train_texts, train_slot_tags, train_pos_tags, train_labels, word_to_ix, tag_to_ix, pos_to_ix)\n",
        "dev_dataset = DatasetObj(dev_texts, dev_slot_tags, dev_pos_tags, dev_labels, word_to_ix, tag_to_ix, pos_to_ix)\n",
        "test_dataset = DatasetObj(test_texts, test_slot_tags, test_pos_tags, test_labels, word_to_ix, tag_to_ix, pos_to_ix)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=DatasetObj.collate_fn)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=DatasetObj.collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=DatasetObj.collate_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Training and Optimizer Initialization\n",
        "\n",
        "Initializes the model with the specified vocabulary sizes, embedding dimensions, and other hyperparameters. It also creates an optimizer for updating the model's weights based on the computed gradients during training.\n",
        "\n",
        "Calls the train_and_validate_model function to train the model and print the best validation loss achieved."
      ],
      "metadata": {
        "id": "EwwqlLDmY2ot"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "KfCGecCUhIpY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d6c9b6-acd0-499f-d301-a974f2fdecf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 0.9996, Validation Loss: 0.6752\n",
            "Model saved with improved validation loss.\n",
            "Epoch 2, Train Loss: 0.5395, Validation Loss: 0.6275\n",
            "Model saved with improved validation loss.\n",
            "Epoch 3, Train Loss: 0.5001, Validation Loss: 0.6020\n",
            "Model saved with improved validation loss.\n",
            "Epoch 4, Train Loss: 0.4786, Validation Loss: 0.5885\n",
            "Model saved with improved validation loss.\n",
            "Epoch 5, Train Loss: 0.4672, Validation Loss: 0.5966\n",
            "Epoch 6, Train Loss: 0.4610, Validation Loss: 0.5823\n",
            "Model saved with improved validation loss.\n",
            "Epoch 7, Train Loss: 0.4541, Validation Loss: 0.5749\n",
            "Model saved with improved validation loss.\n",
            "Epoch 8, Train Loss: 0.4477, Validation Loss: 0.5838\n",
            "Epoch 9, Train Loss: 0.4443, Validation Loss: 0.5712\n",
            "Model saved with improved validation loss.\n",
            "Epoch 10, Train Loss: 0.4415, Validation Loss: 0.5684\n",
            "Model saved with improved validation loss.\n",
            "Epoch 11, Train Loss: 0.4382, Validation Loss: 0.5865\n",
            "Epoch 12, Train Loss: 0.4358, Validation Loss: 0.5809\n",
            "Epoch 13, Train Loss: 0.4325, Validation Loss: 0.5694\n",
            "Epoch 14, Train Loss: 0.4298, Validation Loss: 0.5728\n",
            "Epoch 15, Train Loss: 0.4277, Validation Loss: 0.5733\n",
            "Best Validation Loss: 0.5683610962629319\n"
          ]
        }
      ],
      "source": [
        "pos_vocab_size = calculate_max_pos_vocab_size([train_loader, dev_loader, test_loader])\n",
        "model = Model(vocab_size=vocab_size,\n",
        "              tagset_size=tagset_size,\n",
        "              embedding_dim=embedding_dim,\n",
        "              hidden_dim=hidden_dim,\n",
        "              pretrained_word_embeddings=pretrained_embeddings,\n",
        "              pretrained_pos_embeddings=pos_embeddings,\n",
        "              pos_vocab_size=pos_vocab_size,\n",
        "              pos_embedding_dim=pos_embedding_dim,\n",
        "              num_labels=num_labels,\n",
        "              num_lstm_layers=num_lstm_layers).to(device)\n",
        "\n",
        "# Train the model with the specified hyperparameters and data loaders.\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Optimizer setup.\n",
        "# Call the training and validation function, passing the necessary arguments.\n",
        "val_loss = train_and_validate_model(model, train_loader, dev_loader, optimizer, num_epochs, device, **tradeoff_params)\n",
        "# Output the best validation loss achieved during training.\n",
        "if val_loss < best_val_loss:\n",
        "   best_val_loss = val_loss\n",
        "print(f\"Best Validation Loss: {best_val_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Testing\n",
        "\n",
        "It evaluates the trained model's performance on the test dataset using the evaluate_model function."
      ],
      "metadata": {
        "id": "j1uWB8JYZas7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "IbfDULgBASB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68a99ad8-dd13-461c-b612-d5b78926c392"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slot Filling - Accuracy: 0.9475, Precision: 0.8268, Recall: 0.8021, F1: 0.8092\n",
            "Intent Detection - Accuracy: 0.7223, Precision: 0.0856, Recall: 0.0728, F1: 0.0749\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(model, test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}