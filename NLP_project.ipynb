{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU1rf5RTofCZ",
        "outputId": "9a340121-7910-42a6-ee61-ec5fb61ec51b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAeVP1Mqr9Ky",
        "outputId": "d405175f-c38f-444b-cd98-e800b6104deb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting pytorch-crf\n",
            "  Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: pytorch-crf\n",
            "Successfully installed pytorch-crf-0.7.2\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (2.2.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext) (2.0.7)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->torchtext) (12.4.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->torchtext) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->torchtext) (1.3.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install pytorch-crf\n",
        "!pip install torchtext\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2xRHzlGL1G95"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "\n",
        "\n",
        "from torchcrf import CRF\n",
        "from torchtext.vocab import GloVe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ac3O40jCY1GO",
        "outputId": "dff7e908-3ddd-4512-92bc-c824417209f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdgR-stHP6Ag",
        "outputId": "9e4d90f4-60d4-4c48-d16d-1a62427028c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# Define device for training based on CUDA availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8qr7dpWi3Jh7"
      },
      "outputs": [],
      "source": [
        "def extract_data(seq_in_path, seq_out_path, label_path):\n",
        "    with open(seq_in_path, 'r', encoding='utf-8') as seq_in_file, \\\n",
        "         open(seq_out_path, 'r', encoding='utf-8') as seq_out_file, \\\n",
        "         open(label_path, 'r', encoding='utf-8') as label_file:\n",
        "\n",
        "        texts = [line.strip().split() for line in seq_in_file.readlines()]\n",
        "        slot_tags = [line.strip().split() for line in seq_out_file.readlines()]\n",
        "        labels = [line.strip() for line in label_file.readlines()]\n",
        "\n",
        "    return texts, slot_tags, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xrObCoAB3JuA"
      },
      "outputs": [],
      "source": [
        "def prepare_data(texts, slot_tags):\n",
        "    word_to_ix = {\"<PAD>\": 0}\n",
        "    tag_to_ix = {\"<PAD>\": 0}\n",
        "\n",
        "    for sentence in texts:\n",
        "        for word in sentence:\n",
        "            if word not in word_to_ix:\n",
        "                word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "    for tags in slot_tags:\n",
        "        for tag in tags:\n",
        "            if tag not in tag_to_ix:\n",
        "                tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "    return word_to_ix, tag_to_ix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zSGWqX6n3KSl"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class DatasetObj(Dataset):\n",
        "    def __init__(self, texts, slot_tags, labels=None, word_to_ix=None, tag_to_ix=None):\n",
        "        self.texts = texts\n",
        "        self.slot_tags = slot_tags\n",
        "        self.labels = labels\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "\n",
        "        self.sentences = [[self.word_to_ix[word] for word in sentence] for sentence in texts]\n",
        "        self.tags = [[self.tag_to_ix[tag] for tag in slot_tag] for slot_tag in slot_tags]\n",
        "        if labels is not None:\n",
        "            self.label_to_ix = {label: i for i, label in enumerate(set(labels))}\n",
        "            self.labels_ix = [self.label_to_ix[label] for label in labels]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence_tensor = torch.tensor(self.sentences[idx], dtype=torch.long)\n",
        "        tags_tensor = torch.tensor(self.tags[idx], dtype=torch.long)\n",
        "\n",
        "        if self.labels is not None:\n",
        "            label_tensor = torch.tensor(self.labels_ix[idx], dtype=torch.long)\n",
        "            return sentence_tensor, tags_tensor, label_tensor\n",
        "        else:\n",
        "            return sentence_tensor, tags_tensor\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        sentences, tags, *optional_labels = zip(*batch)\n",
        "        sentences_padded = torch.nn.utils.rnn.pad_sequence(sentences, batch_first=True, padding_value=0)\n",
        "        tags_padded = torch.nn.utils.rnn.pad_sequence(tags, batch_first=True, padding_value=0)\n",
        "\n",
        "        if optional_labels:\n",
        "            labels = torch.tensor(optional_labels[0], dtype=torch.long)\n",
        "            return sentences_padded, tags_padded, labels\n",
        "        else:\n",
        "            return sentences_padded, tags_padded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bd7Y3ph83Kaw"
      },
      "outputs": [],
      "source": [
        "def print_examples(texts, slot_tags, labels=None, num_examples=5):\n",
        "    for i in range(min(num_examples, len(texts))):\n",
        "        print(\"Sentence:\", \" \".join(texts[i]))\n",
        "        print(\"Tags:\", \" \".join(slot_tags[i]))\n",
        "        if labels is not None:\n",
        "            print(\"Label:\", labels[i])\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1r6NguiPmz8J"
      },
      "outputs": [],
      "source": [
        "# Choose dataset (atis, snips):\n",
        "ds = \"atis\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rG0Yjuj8ml26",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Paths for the training dataset\n",
        "train_seq_in_path = f'/content/drive/MyDrive/data/{ds}/train/seq.in'\n",
        "train_seq_out_path = f'/content/drive/MyDrive/data/{ds}/train/seq.out'\n",
        "train_label_path = f'/content/drive/MyDrive/data/{ds}/train/label'\n",
        "\n",
        "# Paths for the development dataset\n",
        "dev_seq_in_path = f'/content/drive/MyDrive/data/{ds}/dev/seq.in'\n",
        "dev_seq_out_path = f'/content/drive/MyDrive/data/{ds}/dev/seq.out'\n",
        "dev_label_path = f'/content/drive/MyDrive/data/{ds}/dev/label'\n",
        "\n",
        "# Paths for the test dataset\n",
        "test_seq_in_path = f'/content/drive/MyDrive/data/{ds}/test/seq.in'\n",
        "test_seq_out_path = f'/content/drive/MyDrive/data/{ds}/test/seq.out'\n",
        "test_label_path = f'/content/drive/MyDrive/data/{ds}/test/label'\n",
        "\n",
        "# Extract data for each split\n",
        "train_texts, train_slot_tags, train_labels = extract_data(train_seq_in_path, train_seq_out_path, train_label_path)\n",
        "dev_texts, dev_slot_tags, dev_labels = extract_data(dev_seq_in_path, dev_seq_out_path, dev_label_path)\n",
        "test_texts, test_slot_tags, test_labels = extract_data(test_seq_in_path, test_seq_out_path, test_label_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iVZ6J4a7orpJ"
      },
      "outputs": [],
      "source": [
        "all_texts = train_texts + dev_texts + test_texts\n",
        "all_slot_tags = train_slot_tags + dev_slot_tags + test_slot_tags\n",
        "\n",
        "word_to_ix, tag_to_ix = prepare_data(all_texts, all_slot_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI_6WuOOSn-A",
        "outputId": "e7b02a8e-445d-4fe5-848b-57a6c00f45e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:52, 5.01MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:54<00:00, 7275.75it/s]\n"
          ]
        }
      ],
      "source": [
        "glove = GloVe(name='6B', dim=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "CpEqXq6VSqPq"
      },
      "outputs": [],
      "source": [
        "def create_embedding_matrix(word_to_ix, glove):\n",
        "    embedding_dim = glove.dim\n",
        "    embeddings = torch.randn(len(word_to_ix), embedding_dim)\n",
        "    embeddings[word_to_ix[\"<PAD>\"]] = torch.zeros(embedding_dim)  # Zero embedding for padding\n",
        "\n",
        "    for word, ix in word_to_ix.items():\n",
        "        if word in glove.stoi:\n",
        "            embeddings[ix] = glove[word]\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "pretrained_embeddings = create_embedding_matrix(word_to_ix, glove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2QCrMVoJLrfe"
      },
      "outputs": [],
      "source": [
        "class SentenceLevelPrediction(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_labels):\n",
        "        super(SentenceLevelPrediction, self).__init__()\n",
        "        self.pooling = nn.AdaptiveMaxPool1d(1)\n",
        "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
        "\n",
        "    def forward(self, lstm_out):\n",
        "        pooled = self.pooling(lstm_out.transpose(1, 2)).squeeze(-1)\n",
        "        logits = self.fc(pooled)\n",
        "        return torch.sigmoid(logits)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NMwC-PuaLADm"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim * 2, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Pkk73YzG3K3S"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim, pretrained_embeddings=None, num_labels=None):\n",
        "        super(Model, self).__init__()\n",
        "        if pretrained_embeddings is not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "        self.crf = CRF(tagset_size, batch_first=True)\n",
        "\n",
        "        # Adversarial MI Estimation Components\n",
        "        self.discriminator = Discriminator(hidden_dim)\n",
        "\n",
        "        # Initialize the sentence-level prediction module if num_labels is provided\n",
        "        if num_labels is not None:\n",
        "            self.sentence_level_predictor = nn.Linear(hidden_dim, num_labels)\n",
        "\n",
        "        # Auxiliary components\n",
        "        self.word_context_classifier = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "        self.context2label = nn.Linear(hidden_dim, tagset_size)  # <-- New code\n",
        "\n",
        "        self.sentence_label_classifier = nn.Linear(hidden_dim, tagset_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.embedding(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        tag_space = self.hidden2tag(lstm_out)\n",
        "\n",
        "        sentence_representation = torch.mean(lstm_out, dim=1)  # Mean pooling as an example\n",
        "        sentence_level_logits = None\n",
        "        if hasattr(self, 'sentence_level_predictor'):\n",
        "            sentence_level_logits = torch.sigmoid(self.sentence_level_predictor(sentence_representation))\n",
        "\n",
        "        word_labels_from_context = self.context2label(torch.mean(lstm_out, dim=1))  # <-- New code\n",
        "\n",
        "        return tag_space, lstm_out, sentence_level_logits, word_labels_from_context  # <-- Modified return statement\n",
        "\n",
        "    def compute_discriminator_loss(self, lstm_out, device):\n",
        "        batch_size, seq_len, hidden_dim = lstm_out.size()\n",
        "\n",
        "        # Simple context vector generation\n",
        "        context_vectors = lstm_out.mean(dim=1, keepdim=True).expand(-1, seq_len, -1)\n",
        "\n",
        "        joint_samples = torch.cat((lstm_out, context_vectors), dim=-1)\n",
        "\n",
        "        # Randomly shuffle context vectors for marginal samples\n",
        "        idx = torch.randperm(batch_size)\n",
        "        marginal_context_vectors = context_vectors[idx]\n",
        "\n",
        "        marginal_samples = torch.cat((lstm_out, marginal_context_vectors), dim=-1)\n",
        "\n",
        "        # Compute discriminator loss\n",
        "        true_preds = self.discriminator(joint_samples.view(-1, hidden_dim * 2))\n",
        "        false_preds = self.discriminator(marginal_samples.view(-1, hidden_dim * 2))\n",
        "\n",
        "        true_labels = torch.ones(true_preds.size(), device=device)\n",
        "        false_labels = torch.zeros(false_preds.size(), device=device)\n",
        "\n",
        "        disc_loss = F.binary_cross_entropy(torch.cat((true_preds, false_preds), dim=0),\n",
        "                                           torch.cat((true_labels, false_labels), dim=0))\n",
        "\n",
        "        return disc_loss\n",
        "\n",
        "    def compute_auxiliary_losses(self, lstm_out, tags, sentence_lengths, labels=None):\n",
        "        # Initialization of loss components\n",
        "        auxiliary_loss = 0\n",
        "\n",
        "        lstm_out_flat = lstm_out.contiguous().view(-1, lstm_out.shape[-1])\n",
        "        tags_flat = tags.contiguous().view(-1)\n",
        "        word_context_logits = self.word_context_classifier(lstm_out_flat)\n",
        "        word_context_loss = nn.CrossEntropyLoss()(word_context_logits, tags_flat)\n",
        "        auxiliary_loss += word_context_loss\n",
        "\n",
        "        if labels is not None and hasattr(self, 'sentence_level_predictor'):\n",
        "            sentence_representation = torch.mean(lstm_out, dim=1)\n",
        "            sentence_label_logits = self.sentence_level_predictor(sentence_representation)\n",
        "            sentence_label_loss = nn.BCEWithLogitsLoss()(sentence_label_logits, labels.float())\n",
        "            auxiliary_loss += sentence_label_loss\n",
        "\n",
        "        return auxiliary_loss\n",
        "\n",
        "\n",
        "    def loss(self, tag_space, lstm_out, sentence_level_logits, word_labels_from_context, tags, alpha, beta, gamma, labels=None):\n",
        "        crf_loss = -self.crf(tag_space, tags, mask=(tags != 0), reduction='mean')\n",
        "        disc_loss = self.compute_discriminator_loss(lstm_out, device)\n",
        "\n",
        "        auxiliary_loss = 0\n",
        "        if labels is not None:\n",
        "            if sentence_level_logits is not None:\n",
        "                sentence_level_loss = F.binary_cross_entropy_with_logits(sentence_level_logits, labels.float())\n",
        "                auxiliary_loss += sentence_level_loss\n",
        "\n",
        "        total_loss = alpha * crf_loss + beta * disc_loss + gamma * auxiliary_loss\n",
        "        return total_loss\n",
        "\n",
        "\n",
        "    def predict(self, sentences):\n",
        "        tag_space, _, _, _ = self.forward(sentences)\n",
        "        tags = self.crf.decode(tag_space)\n",
        "        return tags\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "G-LytzJdASFf"
      },
      "outputs": [],
      "source": [
        "def train_and_validate_model(model, train_loader, dev_loader, optimizer, num_epochs, device, alpha, beta, gamma):\n",
        "    best_validation_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for sentences, tags, labels in train_loader:\n",
        "            sentences, tags = sentences.to(device), tags.to(device)\n",
        "            labels = labels.to(device) if labels is not None else None\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Corrected: Ensure variables are defined by capturing the model's forward method outputs\n",
        "            outputs = model(sentences)  # Call the model's forward method\n",
        "            tag_space, lstm_out, sentence_level_logits, word_labels_from_context = outputs\n",
        "\n",
        "            # Now use the outputs in the model's loss function\n",
        "            loss = model.loss(tag_space=tag_space, lstm_out=lstm_out, sentence_level_logits=sentence_level_logits,\n",
        "                  word_labels_from_context=word_labels_from_context, tags=tags,\n",
        "                  alpha=alpha, beta=beta, gamma=gamma, labels=labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        # Calculate average train loss\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        total_validation_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for sentences, tags, labels in dev_loader:\n",
        "                sentences, tags = sentences.to(device), tags.to(device)\n",
        "                labels = labels.to(device) if labels is not None else None\n",
        "\n",
        "                # Compute the loss for validation, similarly updated\n",
        "                tag_space, lstm_out, sentence_level_logits, word_labels_from_context = model(sentences)  # <-- Updated call for validation\n",
        "                val_loss = model.loss(tag_space, lstm_out, sentence_level_logits, word_labels_from_context,  # <-- New validation parameters\n",
        "                                      tags, alpha, beta, gamma, labels=labels)  # Adjusted for validation\n",
        "                total_validation_loss += val_loss.item()\n",
        "\n",
        "\n",
        "        # Calculate average validation loss\n",
        "        avg_validation_loss = total_validation_loss / len(dev_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_validation_loss:.4f}\")\n",
        "\n",
        "        # Save the model if validation loss has improved\n",
        "        if avg_validation_loss < best_validation_loss:\n",
        "            best_validation_loss = avg_validation_loss\n",
        "            torch.save(model.state_dict(), f'model_best_validation.pth')\n",
        "            print(\"Model saved with improved validation loss.\")\n",
        "\n",
        "    return best_validation_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pPGMuisSdNAO"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    true_tags, pred_tags = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            try:\n",
        "                sentences, tags, _ = batch  # Attempt to unpack as if labels are present\n",
        "            except ValueError:\n",
        "                sentences, tags = batch  # Fallback if only sentences and tags are returned\n",
        "\n",
        "            sentences, tags = sentences.to(device), tags.to(device)\n",
        "\n",
        "            predicted_tags_batch = model.predict(sentences)\n",
        "            # Convert predicted tags to the same format as true_tags for evaluation\n",
        "            predicted_tags_batch = torch.tensor(predicted_tags_batch, dtype=torch.long, device=device)\n",
        "            predicted_tags_batch = predicted_tags_batch.view(-1).cpu().numpy()\n",
        "\n",
        "            true_tags.extend(tags.view(-1).cpu().numpy())\n",
        "            pred_tags.extend(predicted_tags_batch)\n",
        "\n",
        "    # Calculate evaluation metrics here, such as accuracy, and print them out\n",
        "    accuracy = accuracy_score(true_tags, pred_tags)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(true_tags, pred_tags, average='macro', zero_division=1)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Hs_SNF53OkVy"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(word_to_ix)\n",
        "tagset_size = len(tag_to_ix)\n",
        "\n",
        "embedding_dim = 200\n",
        "hidden_dim = 200\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_epochs = 15\n",
        "batch_size = 1\n",
        "\n",
        "tradeoff_params = {'alpha': 1.0, 'beta': 1.0, 'gamma': 1.0}\n",
        "\n",
        "best_val_loss = float('inf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "QaeeaiCVhICN"
      },
      "outputs": [],
      "source": [
        "train_dataset = DatasetObj(train_texts, train_slot_tags, train_labels, word_to_ix, tag_to_ix)\n",
        "dev_dataset = DatasetObj(dev_texts, dev_slot_tags, dev_labels, word_to_ix, tag_to_ix)\n",
        "test_dataset = DatasetObj(test_texts, test_slot_tags, test_labels, word_to_ix, tag_to_ix)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=DatasetObj.collate_fn)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=DatasetObj.collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=DatasetObj.collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "KfCGecCUhIpY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d5b1eee-1ab9-4bac-d5f2-593547f7a32d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 5.5936, Validation Loss: 3.8964\n",
            "Model saved with improved validation loss.\n",
            "Epoch 2, Train Loss: 3.0887, Validation Loss: 3.3431\n",
            "Model saved with improved validation loss.\n",
            "Epoch 3, Train Loss: 2.7422, Validation Loss: 3.1388\n",
            "Model saved with improved validation loss.\n",
            "Epoch 4, Train Loss: 2.5867, Validation Loss: 3.0070\n",
            "Model saved with improved validation loss.\n",
            "Epoch 5, Train Loss: 2.4915, Validation Loss: 3.0386\n",
            "Epoch 6, Train Loss: 2.4312, Validation Loss: 2.9683\n",
            "Model saved with improved validation loss.\n",
            "Epoch 7, Train Loss: 2.3867, Validation Loss: 3.0379\n",
            "Epoch 8, Train Loss: 2.3512, Validation Loss: 2.9707\n",
            "Epoch 9, Train Loss: 2.3286, Validation Loss: 3.0192\n",
            "Epoch 10, Train Loss: 2.3098, Validation Loss: 2.9732\n",
            "Epoch 11, Train Loss: 2.2842, Validation Loss: 3.0276\n",
            "Epoch 12, Train Loss: 2.2749, Validation Loss: 3.0432\n",
            "Epoch 13, Train Loss: 2.2578, Validation Loss: 3.0664\n",
            "Epoch 14, Train Loss: 2.2463, Validation Loss: 3.1224\n",
            "Epoch 15, Train Loss: 2.2376, Validation Loss: 3.0710\n",
            "Epoch 16, Train Loss: 2.2252, Validation Loss: 3.0859\n",
            "Epoch 17, Train Loss: 2.2258, Validation Loss: 3.1303\n",
            "Epoch 18, Train Loss: 2.2182, Validation Loss: 3.1827\n",
            "Epoch 19, Train Loss: 2.2080, Validation Loss: 3.2306\n",
            "Epoch 20, Train Loss: 2.2060, Validation Loss: 3.2326\n",
            "Epoch 21, Train Loss: 2.1987, Validation Loss: 3.2455\n",
            "Epoch 22, Train Loss: 2.1907, Validation Loss: 3.2670\n",
            "Epoch 23, Train Loss: 2.1897, Validation Loss: 3.2921\n",
            "Epoch 24, Train Loss: 2.1809, Validation Loss: 3.3079\n",
            "Epoch 25, Train Loss: 2.1747, Validation Loss: 3.3656\n",
            "Epoch 26, Train Loss: 2.1777, Validation Loss: 3.3781\n",
            "Epoch 27, Train Loss: 2.1708, Validation Loss: 3.4055\n",
            "Epoch 28, Train Loss: 2.1721, Validation Loss: 3.4064\n",
            "Epoch 29, Train Loss: 2.1647, Validation Loss: 3.4741\n",
            "Epoch 30, Train Loss: 2.1643, Validation Loss: 3.4632\n",
            "Epoch 31, Train Loss: 2.1646, Validation Loss: 3.4383\n",
            "Epoch 32, Train Loss: 2.1612, Validation Loss: 3.4741\n",
            "Epoch 33, Train Loss: 2.1592, Validation Loss: 3.5236\n",
            "Epoch 34, Train Loss: 2.1527, Validation Loss: 3.5253\n",
            "Epoch 35, Train Loss: 2.1566, Validation Loss: 3.5108\n",
            "Epoch 36, Train Loss: 2.1558, Validation Loss: 3.5506\n",
            "Epoch 37, Train Loss: 2.1576, Validation Loss: 3.5692\n",
            "Epoch 38, Train Loss: 2.1534, Validation Loss: 3.5932\n",
            "Epoch 39, Train Loss: 2.1519, Validation Loss: 3.6495\n",
            "Epoch 40, Train Loss: 2.1500, Validation Loss: 3.6902\n",
            "Epoch 41, Train Loss: 2.1476, Validation Loss: 3.7055\n",
            "Epoch 42, Train Loss: 2.1478, Validation Loss: 3.7247\n",
            "Epoch 43, Train Loss: 2.1448, Validation Loss: 3.7606\n",
            "Epoch 44, Train Loss: 2.1429, Validation Loss: 3.7381\n",
            "Epoch 45, Train Loss: 2.1452, Validation Loss: 3.8080\n",
            "Epoch 46, Train Loss: 2.1438, Validation Loss: 3.8262\n",
            "Epoch 47, Train Loss: 2.1413, Validation Loss: 3.8440\n",
            "Epoch 48, Train Loss: 2.1419, Validation Loss: 3.8680\n",
            "Epoch 49, Train Loss: 2.1378, Validation Loss: 3.8560\n",
            "Epoch 50, Train Loss: 2.1386, Validation Loss: 3.8912\n",
            "Best Validation Loss: 2.9682805082798005\n"
          ]
        }
      ],
      "source": [
        "model = Model(vocab_size, tagset_size, embedding_dim, hidden_dim, pretrained_embeddings=pretrained_embeddings).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "val_loss = train_and_validate_model(model, train_loader, dev_loader, optimizer, num_epochs, device, **tradeoff_params)\n",
        "\n",
        "if val_loss < best_val_loss:\n",
        "   best_val_loss = val_loss\n",
        "\n",
        "print(f\"Best Validation Loss: {best_val_loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "IbfDULgBASB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8643393-780e-430a-a06f-7cfc80c37e09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9045\n",
            "Precision: 0.7032, Recall: 0.5471, F1: 0.4793\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(model, test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}